{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":492536,"sourceType":"datasetVersion","datasetId":230545},{"sourceId":451254,"sourceType":"modelInstanceVersion","modelInstanceId":366101,"modelId":386993},{"sourceId":453977,"sourceType":"modelInstanceVersion","modelInstanceId":368334,"modelId":389212}],"dockerImageVersionId":31040,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ✅ General Purpose\nimport os\nimport glob\nimport random\nfrom pathlib import Path\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# ✅ Image Processing\nimport cv2\nfrom scipy.io import loadmat\nfrom scipy.ndimage import gaussian_filter\n\n# ✅ PyTorch and Neural Network\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision\nimport torchvision.transforms as T\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:40.953958Z","iopub.execute_input":"2025-06-30T11:09:40.954582Z","iopub.status.idle":"2025-06-30T11:09:49.194859Z","shell.execute_reply.started":"2025-06-30T11:09:40.954557Z","shell.execute_reply":"2025-06-30T11:09:49.194122Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path_img_ex = '../input/shanghaitech/ShanghaiTech/part_B/train_data/images/IMG_6.jpg'\nimage_ex = cv2.cvtColor(cv2.imread(path_img_ex),cv2.COLOR_BGR2RGB)\nfigure = plt.figure(figsize=(5,5))\nplt.imshow(image_ex)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:49.196144Z","iopub.execute_input":"2025-06-30T11:09:49.196621Z","iopub.status.idle":"2025-06-30T11:09:49.590434Z","shell.execute_reply.started":"2025-06-30T11:09:49.196594Z","shell.execute_reply":"2025-06-30T11:09:49.589754Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"path_gt_ex = \"../input/shanghaitech/ShanghaiTech/part_B/train_data/ground-truth/GT_IMG_6.mat\"\ngt_ex = loadmat(path_gt_ex)\nprint('type: ', type(gt_ex))\nprint(gt_ex.items())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:52.63047Z","iopub.execute_input":"2025-06-30T11:09:52.63095Z","iopub.status.idle":"2025-06-30T11:09:52.659602Z","shell.execute_reply.started":"2025-06-30T11:09:52.630924Z","shell.execute_reply":"2025-06-30T11:09:52.659015Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(gt_ex.keys())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:53.024823Z","iopub.execute_input":"2025-06-30T11:09:53.025137Z","iopub.status.idle":"2025-06-30T11:09:53.029522Z","shell.execute_reply.started":"2025-06-30T11:09:53.025113Z","shell.execute_reply":"2025-06-30T11:09:53.0288Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"gt_coor_ex = gt_ex.get('image_info')[0][0][0][0][0]\nprint('Shape of coordinates: ', gt_coor_ex.shape)\n#print(gt_coor_ex)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:55.955627Z","iopub.execute_input":"2025-06-30T11:09:55.955897Z","iopub.status.idle":"2025-06-30T11:09:55.960423Z","shell.execute_reply.started":"2025-06-30T11:09:55.955876Z","shell.execute_reply":"2025-06-30T11:09:55.959538Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"figure = plt.figure(figsize=(5,5))\n\nfor x_cor, y_cor in gt_coor_ex:\n    cv2.drawMarker(image_ex, (int(x_cor), int(y_cor)),(255, 0, 0),thickness=3)\n\nplt.imshow(image_ex)\nplt.title(\"Image and Coordinate\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:57.825194Z","iopub.execute_input":"2025-06-30T11:09:57.825906Z","iopub.status.idle":"2025-06-30T11:09:58.172298Z","shell.execute_reply.started":"2025-06-30T11:09:57.825873Z","shell.execute_reply":"2025-06-30T11:09:58.171297Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nfrom scipy.ndimage import gaussian_filter\n\ndef gen_density_map_gaussian(image, coords, sigma=5, truncate=3):\n    \"\"\"\n    Generate a density map from point coordinates using Gaussian kernel.\n\n    Args:\n        image (np.array): Reference image (only for getting height and width).\n        coords (array-like): Array/List of (x, y) coordinates.\n        sigma (float): Gaussian standard deviation.\n        truncate (float): Truncation factor for kernel size (default: 3).\n\n    Returns:\n        np.array: Density map of shape (H, W).\n    \"\"\"\n    h, w = image.shape[:2]\n    density_map = np.zeros((h, w), dtype=np.float32)\n\n    if len(coords) == 0:\n        return density_map\n\n    for x, y in coords:\n        x = int(x)\n        y = int(y)\n\n        if 0 <= x < w and 0 <= y < h:\n            density_map[y, x] = 1\n\n    # Apply Gaussian filter for smoothing\n    density_map = gaussian_filter(density_map, sigma=sigma, mode='constant', truncate=truncate)\n\n    # Optional normalization: Uncomment if you want total density sum ≈ number of people\n    if density_map.sum() > 0:\n        density_map = density_map * (len(coords) / density_map.sum())\n\n    return density_map\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:09:58.177433Z","iopub.execute_input":"2025-06-30T11:09:58.177711Z","iopub.status.idle":"2025-06-30T11:09:58.183886Z","shell.execute_reply.started":"2025-06-30T11:09:58.177682Z","shell.execute_reply":"2025-06-30T11:09:58.183041Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"density_map_ex = gen_density_map_gaussian(image_ex, gt_coor_ex, 5)\n\nfigure = plt.figure(figsize=(10,5))\nplt.subplot(1,2,1)\nimage_ex = torch.tensor(image_ex/255, dtype=torch.float)\nplt.xlabel(image_ex.shape)\nplt.title('GT: '+str(gt_coor_ex.shape[0]))\nplt.imshow(image_ex)\n\nplt.subplot(1,2,2)\nplt.xlabel(density_map_ex.shape)\nplt.title('DM: '+str(np.sum(density_map_ex)))\nplt.imshow(density_map_ex, cmap=\"jet\")\n\nprint('max1 : ', image_ex.max())\nprint('max2 : ', density_map_ex.max())\nprint('min1 : ', image_ex.min())\nprint('min2 : ', density_map_ex.min())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:01.993754Z","iopub.execute_input":"2025-06-30T11:10:01.994035Z","iopub.status.idle":"2025-06-30T11:10:02.659109Z","shell.execute_reply.started":"2025-06-30T11:10:01.994009Z","shell.execute_reply":"2025-06-30T11:10:02.658468Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchvision.transforms as T\nimport numpy as np\nimport cv2\nimport os\nimport random\nfrom torch.utils.data import Dataset\nfrom scipy.io import loadmat\nfrom scipy.ndimage import gaussian_filter\n\nclass EnhancedShanghaiTechDataset(Dataset):\n    def __init__(self, root_dir, gt_downsample=4, shuffle=False, sigma_base=5, augment=False, target_size=(512, 512), normalize_dm=True):\n        self.root_dir = root_dir\n        self.gt_downsample = gt_downsample\n        self.shuffle = shuffle\n        self.sigma_base = sigma_base\n        self.augment = augment\n        self.target_size = target_size  # (Height, Width)\n        self.normalize_dm = normalize_dm\n\n        # ✅ Load image filenames\n        self.img_names = [f for f in os.listdir(os.path.join(root_dir, 'images')) if f.endswith('.jpg')]\n        if self.shuffle:\n            random.shuffle(self.img_names)\n\n        self.n_people = {}\n        self.DMs = {}\n\n        # ✅ Precompute density maps and GT counts\n        for img_name in self.img_names:\n            img_path = os.path.join(root_dir, 'images', img_name)\n            GT_path = os.path.join(root_dir, 'ground-truth', f\"GT_{os.path.splitext(img_name)[0]}.mat\")\n\n            GT = loadmat(GT_path)['image_info'][0, 0][0, 0][0]\n            img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n            resized_img = cv2.resize(img, self.target_size)\n\n            dm = self.generate_density_map(resized_img, GT, original_shape=img.shape[:2])\n            self.DMs[img_path] = dm\n            self.n_people[img_path] = len(GT)\n\n        # ✅ Data augmentation\n        self.augmentation = T.Compose([\n            T.RandomHorizontalFlip(),\n            T.RandomRotation(degrees=10),\n            T.RandomResizedCrop(size=self.target_size, scale=(0.8, 1.0)),\n            T.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1)\n        ])\n\n    def generate_density_map(self, img, points, original_shape):\n        h, w = img.shape[:2]\n        density_map = np.zeros((h, w), dtype=np.float32)\n        h_scale = w / original_shape[1]\n        v_scale = h / original_shape[0]\n\n        for point in points:\n            x = int(point[0] * h_scale)\n            y = int(point[1] * v_scale)\n\n            if x >= w or y >= h or x < 0 or y < 0:\n                continue\n\n            sigma = self.sigma_base * (0.5 if len(points) > 100 else 1)\n            density_map[y, x] = 1\n\n        density_map = gaussian_filter(density_map, sigma=sigma, truncate=3)\n\n        if self.normalize_dm and density_map.sum() > 0 and len(points) > 0:\n            density_map *= (len(points) / density_map.sum())\n\n        return density_map\n\n    def __len__(self):\n        return len(self.img_names)\n\n    def __getitem__(self, idx):\n        img_name = self.img_names[idx]\n        img_path = os.path.join(self.root_dir, 'images', img_name)\n        img = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\n        img = cv2.resize(img, self.target_size)\n\n        gt_density_map = self.DMs[img_path]\n        gt_n_people = self.n_people[img_path]\n\n        # ✅ Augmentation (PIL-based for torchvision transforms)\n        if self.augment:\n            img = T.ToPILImage()(img)\n            img = self.augmentation(img)\n            img = np.array(img)\n\n        # ✅ Downsample\n        h, w = img.shape[:2]\n        ds_h, ds_w = h // self.gt_downsample, w // self.gt_downsample\n\n        img = cv2.resize(img, (ds_w * self.gt_downsample, ds_h * self.gt_downsample))\n        gt_density_map = cv2.resize(gt_density_map, (ds_w, ds_h))\n        gt_density_map = gt_density_map[np.newaxis, :, :] * (self.gt_downsample ** 2)\n\n        # ✅ Convert to torch tensors\n        img_tensor = torch.from_numpy(img.transpose(2, 0, 1)).float() / 255.0\n        dm_tensor = torch.from_numpy(gt_density_map).float()\n\n        return img_tensor, dm_tensor, gt_n_people\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:06.710484Z","iopub.execute_input":"2025-06-30T11:10:06.710987Z","iopub.status.idle":"2025-06-30T11:10:06.725295Z","shell.execute_reply.started":"2025-06-30T11:10:06.710963Z","shell.execute_reply":"2025-06-30T11:10:06.724435Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"root_dir = \"../input/shanghaitech/ShanghaiTech/part_B/test_data/\"\ndataset = EnhancedShanghaiTechDataset(root_dir, gt_downsample=4, shuffle=True, sigma_base=5)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:10.577454Z","iopub.execute_input":"2025-06-30T11:10:10.578006Z","iopub.status.idle":"2025-06-30T11:10:21.219992Z","shell.execute_reply.started":"2025-06-30T11:10:10.577984Z","shell.execute_reply":"2025-06-30T11:10:21.218617Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"for i, (img, gt_dmap, n_people) in enumerate(dataset):\n  plt.figure(figsize=(10, 5))\n  plt.subplot(1,2,1)\n  plt.xlabel(img.shape)\n  plt.title('GT: ' + str(n_people))\n  plt.imshow(img.permute(1, 2, 0))\n\n  plt.subplot(1,2,2)\n  plt.xlabel(gt_dmap.shape)\n  plt.title('DM: ' + str(np.sum(gt_dmap.numpy())))\n  plt.imshow(gt_dmap.permute(1, 2, 0), cmap=\"jet\")\n  plt.show()\n\n  if i > 0:\n    #print('type of img: ', type(img))\n    #print('type of dmap: ', type(gt_dmap))\n    #print('shape of img: ', img.shape)\n    break\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:31.675539Z","iopub.execute_input":"2025-06-30T11:10:31.675807Z","iopub.status.idle":"2025-06-30T11:10:32.44898Z","shell.execute_reply.started":"2025-06-30T11:10:31.675788Z","shell.execute_reply":"2025-06-30T11:10:32.448311Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass FourColumnMDNN(nn.Module):\n    def __init__(self):\n        super(FourColumnMDNN, self).__init__()\n\n        # ✅ Very Large Receptive Field Column\n        self.column1 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=11, padding=5),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, kernel_size=9, padding=4),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=7, padding=3),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n        )\n\n        # ✅ Large Receptive Field Column\n        self.column2 = nn.Sequential(\n            nn.Conv2d(3, 16, kernel_size=9, padding=4),\n            nn.BatchNorm2d(16),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(16, 32, kernel_size=7, padding=3),\n            nn.BatchNorm2d(32),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(32, 64, kernel_size=5, padding=2),\n            nn.BatchNorm2d(64),\n            nn.ReLU(),\n        )\n\n        # ✅ Medium Receptive Field Column\n        self.column3 = nn.Sequential(\n            nn.Conv2d(3, 20, kernel_size=7, padding=3),\n            nn.BatchNorm2d(20),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(20, 40, kernel_size=5, padding=2),\n            nn.BatchNorm2d(40),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(40, 80, kernel_size=3, padding=1),\n            nn.BatchNorm2d(80),\n            nn.ReLU(),\n        )\n\n        # ✅ Small Receptive Field Column (fine details for dense crowds)\n        self.column4 = nn.Sequential(\n            nn.Conv2d(3, 24, kernel_size=5, padding=2),\n            nn.BatchNorm2d(24),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(24, 48, kernel_size=3, padding=1),\n            nn.BatchNorm2d(48),\n            nn.ReLU(),\n            nn.MaxPool2d(2),\n\n            nn.Conv2d(48, 96, kernel_size=3, padding=1),\n            nn.BatchNorm2d(96),\n            nn.ReLU(),\n        )\n\n        # ✅ Fusion Layer\n        self.fusion = nn.Sequential(\n            nn.Conv2d(64 + 64 + 80 + 96, 64, kernel_size=1),\n            nn.ReLU(),\n            nn.Conv2d(64, 1, kernel_size=1),\n            nn.ReLU()\n        )\n\n    def forward(self, x):\n        x1 = self.column1(x)\n        x2 = self.column2(x)\n        x3 = self.column3(x)\n        x4 = self.column4(x)\n\n        x_cat = torch.cat((x1, x2, x3, x4), dim=1)\n        out = self.fusion(x_cat)\n        return out\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:36.583047Z","iopub.execute_input":"2025-06-30T11:10:36.583581Z","iopub.status.idle":"2025-06-30T11:10:36.594674Z","shell.execute_reply.started":"2025-06-30T11:10:36.583559Z","shell.execute_reply":"2025-06-30T11:10:36.593972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img=torch.rand((1,3,768,1024),dtype=torch.float)\nmcnn=FourColumnMDNN()\nout_dmap=mcnn(img)\nprint(out_dmap.shape)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:42.033476Z","iopub.execute_input":"2025-06-30T11:10:42.034209Z","iopub.status.idle":"2025-06-30T11:10:43.939148Z","shell.execute_reply.started":"2025-06-30T11:10:42.034183Z","shell.execute_reply":"2025-06-30T11:10:43.938438Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport numpy as np\nfrom torch.utils.data import DataLoader, Subset\n\nbatch_size = 8\ndevice = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n\ntrain_root_dir = \"../input/shanghaitech/ShanghaiTech/part_B/train_data/\"\ninit_training_set = EnhancedShanghaiTechDataset(train_root_dir, gt_downsample=4, shuffle=True)\n\n# ✅ Split part of the training set as validation set\ntrain_size = int(0.9 * len(init_training_set))\nval_size = len(init_training_set) - train_size\n\n# Optional: Shuffle indices before split (recommended)\nindices = list(range(len(init_training_set)))\nnp.random.shuffle(indices)\n\ntrain_indices = indices[:train_size]\nval_indices = indices[train_size:]\n\ntrain_dataset = Subset(init_training_set, train_indices)\nval_dataset = Subset(init_training_set, val_indices)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nval_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n\n# ✅ Test set\ntest_root_dir = \"../input/shanghaitech/ShanghaiTech/part_B/test_data/\"\ntest_set = EnhancedShanghaiTechDataset(test_root_dir, gt_downsample=4, shuffle=False)\ntest_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False)\n\nprint(\"Number of batches in train_loader:\", len(train_loader))\nprint(\"Number of batches in val_loader:\", len(val_loader))\nprint(\"Number of batches in test_loader:\", len(test_loader))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:10:47.243713Z","iopub.execute_input":"2025-06-30T11:10:47.244288Z","iopub.status.idle":"2025-06-30T11:11:05.472342Z","shell.execute_reply.started":"2025-06-30T11:10:47.244266Z","shell.execute_reply":"2025-06-30T11:11:05.471644Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np\nimport torch\n\ndef plot_corresponding_pairs(batch1, batch2, plot_map='jet'):\n    num_images = batch1.shape[0]\n    num_cols = 4  # Number of images per row\n    num_rows = int(np.ceil(num_images / num_cols)) * 2  # 2 rows per image (RGB + Density Map)\n\n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, num_rows * 2))\n    axes = np.array(axes).reshape(num_rows, num_cols)  # Handle both 1D and 2D axes cases\n\n    for i in range(num_images):\n        row_rgb = (i // num_cols) * 2\n        col = i % num_cols\n        row_dm = row_rgb + 1\n\n        # ✅ Plot RGB image\n        img_np = batch1[i].cpu().numpy().transpose(1, 2, 0)\n        img_min, img_max = img_np.min(), img_np.max()\n        if img_max - img_min > 1e-5:\n            img_np = (img_np - img_min) / (img_max - img_min)\n        axes[row_rgb, col].imshow(img_np)\n        axes[row_rgb, col].axis('off')\n        axes[row_rgb, col].set_title(f\"Image {i}\")\n\n        # ✅ Plot Density Map\n        dm_np = batch2[i].cpu().squeeze().numpy()\n        axes[row_dm, col].imshow(dm_np, cmap=plot_map)\n        axes[row_dm, col].axis('off')\n        pred_count = np.sum(dm_np)\n        axes[row_dm, col].set_title(f\"Pred Count: {pred_count:.2f}\")\n\n    # ✅ Hide unused subplots\n    total_slots = num_rows * num_cols\n    for j in range(num_images * 2, total_slots):\n        row, col = divmod(j, num_cols)\n        axes[row, col].axis('off')\n\n    plt.tight_layout()\n    plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:11:08.447788Z","iopub.execute_input":"2025-06-30T11:11:08.448061Z","iopub.status.idle":"2025-06-30T11:11:08.455564Z","shell.execute_reply.started":"2025-06-30T11:11:08.44804Z","shell.execute_reply":"2025-06-30T11:11:08.45481Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataiter = iter(train_loader)\nex_images, ex_dmaps, ex_n_people = next(dataiter)\n\n# ✅ Visualize input images and their ground truth density maps\nplot_corresponding_pairs(ex_images, ex_dmaps, plot_map='jet')\n\n# ✅ Print Ground Truth total people count for each image\nprint(\"Ground Truth Counts per Image:\")\nprint(' '.join(f'{ex_n_people[j].item():5.1f}' for j in range(ex_images.size(0))))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:11:13.736817Z","iopub.execute_input":"2025-06-30T11:11:13.737374Z","iopub.status.idle":"2025-06-30T11:11:14.904856Z","shell.execute_reply.started":"2025-06-30T11:11:13.73735Z","shell.execute_reply":"2025-06-30T11:11:14.904127Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\nclass CombinedLoss(nn.Module):\n    def __init__(self, weight_dmap=0.9, weight_sum_gt=0.1):\n        super(CombinedLoss, self).__init__()\n        self.weight_dmap = weight_dmap          # Weight for pixel-wise density map loss\n        self.weight_sum_gt = weight_sum_gt      # Weight for total count loss\n\n        self.img_loss = nn.MSELoss()            # Density map (image-level) loss\n        self.gt_loss_mae = nn.L1Loss()          # Count-level MAE loss\n\n    def forward(self, logits, batch_dmap, batch_gts):\n        batch_gts = batch_gts.float()\n\n        # ✅ Density Map Loss (MSE over pixels)\n        img_loss = self.img_loss(logits, batch_dmap)\n\n        # ✅ Count Loss (MAE between predicted total and GT total)\n        pred_counts = torch.squeeze(logits.sum(dim=(2,3)))\n        gt_loss_mae = self.gt_loss_mae(pred_counts, batch_gts)\n\n        # ✅ Final Combined Loss\n        combined_loss = self.weight_dmap * img_loss + self.weight_sum_gt * gt_loss_mae\n\n        return combined_loss, gt_loss_mae\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:11:26.624323Z","iopub.execute_input":"2025-06-30T11:11:26.624628Z","iopub.status.idle":"2025-06-30T11:11:26.630984Z","shell.execute_reply.started":"2025-06-30T11:11:26.624606Z","shell.execute_reply":"2025-06-30T11:11:26.630137Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import torch\n# import torch.optim as optim\n# import numpy as np\n\n# num_epochs = 200\n# train_losses = []\n# val_losses = []\n# train_mae_losses = []\n# val_mae_losses = []\n\n# model = FourColumnMDNN().to(device)\n# criterion = CombinedLoss(0.9, 0.1)\n# optimizer = optim.Adam(model.parameters(), lr=1e-4)\n\n# best_val_loss = np.inf\n# best_nr_epoch = 0\n\n# for epoch in range(num_epochs):\n#     print(f\"\\nEpoch {epoch+1}/{num_epochs}:\")\n#     model.train()\n#     tr_loss_acc = 0.0\n#     tr_mae_acc = 0.0\n\n#     for batch_img, batch_dmap, batch_gts in train_loader:\n#         batch_img, batch_dmap, batch_gts = batch_img.to(device), batch_dmap.to(device), batch_gts.to(device)\n\n#         optimizer.zero_grad()\n#         logits = model(batch_img)\n#         loss, mae_loss = criterion(logits, batch_dmap, batch_gts)\n#         loss.backward()\n#         optimizer.step()\n\n#         tr_loss_acc += loss.item() * batch_img.size(0)\n#         tr_mae_acc += mae_loss.item() * batch_img.size(0)\n\n#     tr_loss = tr_loss_acc / len(train_loader.dataset)\n#     tr_mae = tr_mae_acc / len(train_loader.dataset)\n\n#     print(f\">> TRAIN: Loss: {tr_loss:.6f}, MAE: {tr_mae:.6f}\")\n\n#     # Validation phase\n#     model.eval()\n#     val_loss_acc = 0.0\n#     val_mae_acc = 0.0\n\n#     with torch.inference_mode():\n#         for batch_img_val, batch_dmap_val, batch_gts_val in val_loader:\n#             batch_img_val, batch_dmap_val, batch_gts_val = batch_img_val.to(device), batch_dmap_val.to(device), batch_gts_val.to(device)\n#             logits = model(batch_img_val)\n#             loss, mae_loss = criterion(logits, batch_dmap_val, batch_gts_val)\n\n#             val_loss_acc += loss.item() * batch_img_val.size(0)\n#             val_mae_acc += mae_loss.item() * batch_img_val.size(0)\n\n#     val_loss = val_loss_acc / len(val_loader.dataset)\n#     val_mae = val_mae_acc / len(val_loader.dataset)\n\n#     print(f\">> VAL:   Loss: {val_loss:.6f}, MAE: {val_mae:.6f}\")\n\n#     # Save best model\n#     if val_loss < best_val_loss:\n#         best_val_loss = val_loss\n#         best_nr_epoch = epoch\n#         torch.save(model.state_dict(), './crowd_counting.pth')\n#         print(f\"✅ Best model saved at epoch {epoch+1} with val_loss {val_loss:.6f}\")\n\n#     # Track history\n#     train_losses.append(tr_loss)\n#     train_mae_losses.append(tr_mae)\n#     val_losses.append(val_loss)\n#     val_mae_losses.append(val_mae)\n\n# print(\"\\n✅ Training Complete!\")\n# print(f\"Best epoch: {best_nr_epoch+1}\")\n# print(f\"Best Train MAE: {train_mae_losses[best_nr_epoch]:.6f}\")\n# print(f\"Best Val MAE:   {val_mae_losses[best_nr_epoch]:.6f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T09:42:06.534001Z","iopub.execute_input":"2025-06-27T09:42:06.534485Z","iopub.status.idle":"2025-06-27T10:40:56.018817Z","shell.execute_reply.started":"2025-06-27T09:42:06.534463Z","shell.execute_reply":"2025-06-27T10:40:56.018099Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# import matplotlib.pyplot as plt\n\n# plt.figure(figsize=(12, 5))\n\n# # ✅ Plot Weighted Loss (Combined Loss)\n# plt.subplot(1, 2, 1)\n# plt.plot(train_losses, label='Training Weighted Loss')\n# plt.plot(val_losses, label='Validation Weighted Loss')\n# plt.title('Training vs Validation Loss')\n# plt.xlabel('Epoch')\n# plt.ylabel('Loss')\n# plt.legend()\n# plt.grid(True)\n\n# # ✅ Plot MAE (Crowd Count Error)\n# plt.subplot(1, 2, 2)\n# plt.plot(train_mae_losses, label='Training MAE')\n# plt.plot(val_mae_losses, label='Validation MAE')\n# plt.title('Training vs Validation MAE')\n# plt.xlabel('Epoch')\n# plt.ylabel('Mean Absolute Error (MAE)')\n# plt.legend()\n# plt.grid(True)\n\n# plt.tight_layout()\n# plt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T10:48:01.629174Z","iopub.execute_input":"2025-06-27T10:48:01.629448Z","iopub.status.idle":"2025-06-27T10:48:02.089637Z","shell.execute_reply.started":"2025-06-27T10:48:01.62943Z","shell.execute_reply":"2025-06-27T10:48:02.088953Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# best_model = FourColumnMDNN().to(device)\n# best_model.load_state_dict(torch.load('./crowd_counting.pth'))\n# best_model.eval()  # ✅ Important: Set model to evaluation mode\n\n# # Get a batch of validation images\n# dataiter = iter(val_loader)\n# ex_images, _, ex_gts = next(dataiter)\n\n# # Run model inference\n# with torch.no_grad():\n#     pred_dms = best_model(ex_images.to(device))\n\n# # Visualize input images and predicted density maps\n# plot_corresponding_pairs(ex_images.cpu(), pred_dms.cpu(), plot_map='twilight')\n\n# # Print ground truth people counts for each image in the batch\n# print('Ground Truth Counts:', ' '.join(f'{ex_gts[j].item():5.1f}' for j in range(ex_images.shape[0])))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-27T10:48:09.539527Z","iopub.execute_input":"2025-06-27T10:48:09.54004Z","iopub.status.idle":"2025-06-27T10:48:10.928407Z","shell.execute_reply.started":"2025-06-27T10:48:09.540017Z","shell.execute_reply":"2025-06-27T10:48:10.927702Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n# ✅ Load the model\nbest_model = FourColumnMDNN().to(device)\nbest_model.load_state_dict(torch.load('/kaggle/input/crowd_counting/tensorflow2/default/1/crowd_counting_MDNN.pth', map_location=device))\nbest_model.eval()\n\ncriterion = nn.L1Loss()\ntest_loss_acc = 0.0\n\n# ✅ Testing loop on full test set\nwith torch.inference_mode():\n    for batch_idx, (batch_img, batch_dmap, batch_gts) in enumerate(test_loader):\n        batch_img, batch_dmap, batch_gts = batch_img.to(device), batch_dmap.to(device), batch_gts.to(device)\n\n        logits = best_model(batch_img)\n        loss = criterion(torch.squeeze(logits.sum(dim=(2,3))), batch_gts)\n\n        test_loss_acc += loss.item()\n\n        # ✅ Optional: Visualize current batch\n        pred_dms = logits.cpu()\n        gt_counts = batch_gts.cpu()\n\n        # Loop through each image in batch\n        for i in range(batch_img.size(0)):\n            fig, axs = plt.subplots(1, 2, figsize=(10, 4))\n\n            # Original image\n            img_np = batch_img[i].cpu().numpy().transpose(1, 2, 0)\n            img_np = (img_np - img_np.min()) / (img_np.max() - img_np.min())  # Normalize for display\n            axs[0].imshow(img_np)\n            axs[0].set_title(f\"Test Image - GT Count: {gt_counts[i].item():.2f}\")\n            axs[0].axis('off')\n\n            # Predicted Density Map\n            density_map = pred_dms[i, 0, :, :].numpy()\n            axs[1].imshow(density_map, cmap='jet')\n            axs[1].set_title(f\"Pred Count: {density_map.sum():.2f}\")\n            axs[1].axis('off')\n\n            plt.tight_layout()\n            plt.show()\n\nprint('TEST: test_MAE: {:.3f}'.format(test_loss_acc / len(test_loader.dataset)))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-30T11:12:56.713625Z","iopub.execute_input":"2025-06-30T11:12:56.714153Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}